{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage import zoom\n",
    "import scipy.ndimage as ndi\n",
    "import imageio\n",
    "import glob\n",
    "%matplotlib inline\n",
    "import math\n",
    "import skimage\n",
    "from skimage.measure import label,regionprops, perimeter\n",
    "from skimage import measure, feature\n",
    "import scipy.misc\n",
    "from matplotlib import patches\n",
    "#os.getcwd()\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log\n",
    "from skimage.filters import median\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic functions for reading raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(csvfname):\n",
    "    # read csv to list of lists\n",
    "    with open(csvfname, 'r') as csvf:\n",
    "        reader = csv.reader(csvf)\n",
    "        csvlines = list(reader)\n",
    "    return csvlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMhd(filename):\n",
    "    # read mhd/raw image\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    scan = sitk.GetArrayFromImage(itkimage) #3D image\n",
    "    spacing = itkimage.GetSpacing() #voxelsize\n",
    "    origin = itkimage.GetOrigin() #world coordinates of origin\n",
    "    transfmat = itkimage.GetDirection() #3D rotation matrix\n",
    "    return scan, spacing, origin, transfmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgWorldTransfMats(spacing,transfmat):\n",
    "    # calc image to world to image transformation matrixes\n",
    "    transfmat = np.array([transfmat[0:3],transfmat[3:6],transfmat[6:9]])\n",
    "    for d in range(3):\n",
    "        transfmat[0:3,d] = transfmat[0:3,d]*spacing[d]\n",
    "    transfmat_toworld = transfmat #image to world coordinates conversion matrix\n",
    "    transfmat_toimg = np.linalg.inv(transfmat) #world to image coordinates conversion matrix\n",
    "    return transfmat_toimg,transfmat_toworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToImgCoord(xyz,origin,transfmat_toimg):\n",
    "    # convert world to image coordinates\n",
    "    xyz = xyz - origin\n",
    "    xyz = np.round(np.matmul(transfmat_toimg,xyz))    \n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmed nodules by at least 2 radiologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv('confirmed.csv', sep=\"\\t\", index_col='LNDbID')\n",
    "print(confirmed.shape)\n",
    "confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOUND = -1000.0 #HU values\n",
    "MAX_BOUND = 400.0 #HU value\n",
    "    \n",
    "def normalize(image):\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1\n",
    "    image[image<0] = 0\n",
    "    image = (image*255).astype('uint8')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "def zero_center(image):\n",
    "    image = image - PIXEL_MEAN\n",
    "    image = (image).astype('uint8')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log(image):\n",
    "    MIN_BOUND = -250\n",
    "    MAX_BOUND = 1500\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    image = (image*255).astype('uint8')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(image):\n",
    "    c = 255 / log(1 + 255) \n",
    "    log_image = c * (np.log(image + 1))  \n",
    "    log_image = np.array(log_image, dtype = np.uint8) \n",
    "    return log_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOUND = -1000.0 #HU values\n",
    "MAX_BOUND = 400.0 #HU value\n",
    "    \n",
    "def normalize(image):\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1\n",
    "    image[image<0] = 0\n",
    "    image = (image*255).astype('uint8')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting 2D RGB images with nodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 1 extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read nodules csv\n",
    "csvlines = readCsv('Train\\\\trainset_csv\\\\trainNodules.csv')\n",
    "header = csvlines[0]\n",
    "nodules = csvlines[1:]\n",
    "\n",
    "for n in nodules:\n",
    "    lnd = int(n[header.index('LNDbID')])\n",
    "    rad = int(n[header.index('RadID')])\n",
    "    finding = int(n[header.index('FindingID')])\n",
    "    \n",
    "    if int(n[header.index('LNDbID')])==lnd and int(n[header.index('RadID')])==rad and int(n[header.index('FindingID')])==finding:\n",
    "        ctr = np.array([float(n[header.index('x')]), float(n[header.index('y')]), float(n[header.index('z')])])\n",
    "        class_label = np.array([int(n[header.index('Nodule')])])\n",
    "        \n",
    "        name = 'LNDb-{:04d}_finding{}_rad{}'.format(lnd,finding,rad)\n",
    "    if name in confirmed.index:\n",
    "    \n",
    "        [scan,spacing,origin,transfmat] = readMhd('Train\\\\train\\\\LNDb-{:04}.mhd'.format(lnd))\n",
    "        transfmat_toimg,transfmat_toworld = getImgWorldTransfMats(spacing,transfmat)\n",
    "        ctr = convertToImgCoord(ctr,origin,transfmat_toimg)\n",
    "\n",
    "        # 2D slice with nodule center\n",
    "        scan_slice = scan[int(ctr[2])]\n",
    "        scan_norm = normalize(scan_slice)\n",
    "        scan_norm_zero = zero_center(scan_norm)\n",
    "\n",
    "        # Previous slice\n",
    "        scan_slice_previous = scan[int(ctr[2])-1]\n",
    "        scan_norm_pre = normalize(scan_slice_previous)\n",
    "        scan_norm_zero_pre = zero_center(scan_norm_pre)\n",
    "        \n",
    "        # Next slice\n",
    "        scan_slice_next = scan[int(ctr[2])+1]\n",
    "        scan_norm_next = normalize(scan_slice_next)\n",
    "        scan_norm_zero_next = zero_center(scan_norm_next)\n",
    "        \n",
    "        # Resize the image\n",
    "        width = 512\n",
    "        height = 512\n",
    "        dim = (height, width)\n",
    "        resized = cv2.resize(scan_norm_zero, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_previous = cv2.resize(scan_norm_zero_pre, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_next = cv2.resize(scan_norm_zero_next, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Create a 3 channel RBG image\n",
    "        R = np.stack((resized_previous, resized, resized_next), axis=2)\n",
    "        \n",
    "        # Save the 3 channel images as jpg                     \n",
    "        #cv2.imwrite('yolov4/median/LNDb-{:04d}_finding{}_rad{}.jpg'.format(lnd,finding,rad), R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 2 extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extract, pre-process and save 2D slice with nodule\n",
    "\n",
    "# Read nodules csv\n",
    "csvlines = readCsv('Train\\\\trainset_csv\\\\trainNodules.csv')\n",
    "header = csvlines[0]\n",
    "nodules = csvlines[1:]\n",
    "\n",
    "for n in nodules:\n",
    "    lnd = int(n[header.index('LNDbID')])\n",
    "    rad = int(n[header.index('RadID')])\n",
    "    finding = int(n[header.index('FindingID')])\n",
    "    \n",
    "    if int(n[header.index('LNDbID')])==lnd and int(n[header.index('RadID')])==rad and int(n[header.index('FindingID')])==finding:\n",
    "        ctr = np.array([float(n[header.index('x')]), float(n[header.index('y')]), float(n[header.index('z')])])\n",
    "        class_label = np.array([int(n[header.index('Nodule')])])\n",
    "        \n",
    "        name = 'LNDb-{:04d}_finding{}_rad{}'.format(lnd,finding,rad)\n",
    "    if name in confirmed.index:\n",
    "\n",
    "        [scan,spacing,origin,transfmat] = readMhd('Train\\\\train\\\\LNDb-{:04}.mhd'.format(lnd))\n",
    "        transfmat_toimg,transfmat_toworld = getImgWorldTransfMats(spacing,transfmat)\n",
    "        ctr = convertToImgCoord(ctr,origin,transfmat_toimg)\n",
    "\n",
    "        # 2D slice with nodule center\n",
    "        scan_slice = scan[int(ctr[2])]\n",
    "        scan_norm = normalize_log(scan_slice)\n",
    "        scan_norm_log = log_transform(scan_norm)\n",
    "\n",
    "        # Previous slice\n",
    "        scan_slice_previous = scan[int(ctr[2])-1]\n",
    "        scan_norm_pre = normalize_log(scan_slice_previous)\n",
    "        scan_norm_log_pre = log_transform(scan_norm_pre)\n",
    " \n",
    "        # Next slice\n",
    "        scan_slice_next = scan[int(ctr[2])+1]\n",
    "        scan_norm_next = normalize_log(scan_slice_next)\n",
    "        scan_norm_log_next = log_transform(scan_norm_next)\n",
    "           \n",
    "        # Resize the image\n",
    "        width = 512\n",
    "        height = 512\n",
    "        dim = (height, width)\n",
    "        resized = cv2.resize(scan_norm_log, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_previous = cv2.resize(scan_norm_log_pre, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_next = cv2.resize(scan_norm_log_next, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Create a 3 channel RBG image\n",
    "        R = np.stack((resized_previous, resized, resized_next), axis=2)\n",
    "        \n",
    "        # Save the 3 channel images as jpg                     \n",
    "        #cv2.imwrite('yolov4/median/LNDb-{:04d}_finding{}_rad{}.jpg'.format(lnd,finding,rad), R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 3 extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract, pre-process and save 2D slice with nodule\n",
    "\n",
    "# Read nodules csv\n",
    "csvlines = readCsv('Train\\\\trainset_csv\\\\trainNodules.csv')\n",
    "header = csvlines[0]\n",
    "nodules = csvlines[1:]\n",
    "\n",
    "for n in nodules:\n",
    "    lnd = int(n[header.index('LNDbID')])\n",
    "    rad = int(n[header.index('RadID')])\n",
    "    finding = int(n[header.index('FindingID')])\n",
    "    \n",
    "    if int(n[header.index('LNDbID')])==lnd and int(n[header.index('RadID')])==rad and int(n[header.index('FindingID')])==finding:\n",
    "        ctr = np.array([float(n[header.index('x')]), float(n[header.index('y')]), float(n[header.index('z')])])\n",
    "        class_label = np.array([int(n[header.index('Nodule')])])\n",
    "        \n",
    "        name = 'LNDb-{:04d}_finding{}_rad{}'.format(lnd,finding,rad)\n",
    "    if name in confirmed.index:\n",
    "\n",
    "        [scan,spacing,origin,transfmat] = readMhd('Train\\\\train\\\\LNDb-{:04}.mhd'.format(lnd))\n",
    "        transfmat_toimg,transfmat_toworld = getImgWorldTransfMats(spacing,transfmat)\n",
    "        ctr = convertToImgCoord(ctr,origin,transfmat_toimg)\n",
    "\n",
    "        # 2D slice with nodule center\n",
    "        scan_slice = scan[int(ctr[2])]\n",
    "        scan_norm = normalize(scan_slice)\n",
    "        scan_norm_med = median(scan_norm)\n",
    "\n",
    "        # Previous slice\n",
    "        scan_slice_previous = scan[int(ctr[2])-1]\n",
    "        scan_norm_pre = normalize(scan_slice_previous)\n",
    "        scan_norm_med_pre = median(scan_norm_pre)\n",
    "\n",
    "        # Next slice\n",
    "        scan_slice_next = scan[int(ctr[2])+1]\n",
    "        scan_norm_next = normalize(scan_slice_next)\n",
    "        scan_norm_med_next = median(scan_norm_next)\n",
    "        \n",
    "        # Resize the image\n",
    "        width = 512\n",
    "        height = 512\n",
    "        dim = (height, width)\n",
    "        resized = cv2.resize(scan_norm_med, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_previous = cv2.resize(scan_norm_med_pre, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized_next = cv2.resize(scan_norm_med_next, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Create a 3 channel RBG image\n",
    "        R = np.stack((resized_previous, resized, resized_next), axis=2)\n",
    "        \n",
    "        # Save the 3 channel images as jpg                     \n",
    "        #cv2.imwrite('yolov4/median/LNDb-{:04d}_finding{}_rad{}.jpg'.format(lnd,finding,rad), R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_conv(s):\n",
    "    I8 = (((s - s.min()) / (s.max() - s.min())) * 255.9).astype(np.uint8)\n",
    "    img = Image.fromarray(I8)\n",
    "    return img\n",
    "\n",
    "#Extract, pre-process and save 2D slice with nodule\n",
    "\n",
    "# Read nodules csv\n",
    "csvlines = readCsv('Train\\\\trainset_csv\\\\trainNodules.csv')\n",
    "header = csvlines[0]\n",
    "nodules = csvlines[1:]\n",
    "\n",
    "for n in nodules:\n",
    "    lnd = int(n[header.index('LNDbID')])\n",
    "    rad = int(n[header.index('RadID')])\n",
    "    finding = int(n[header.index('FindingID')])\n",
    "    \n",
    "    if int(n[header.index('LNDbID')])==lnd and int(n[header.index('RadID')])==rad and int(n[header.index('FindingID')])==finding:\n",
    "        ctr = np.array([float(n[header.index('x')]), float(n[header.index('y')]), float(n[header.index('z')])])\n",
    "        class_label = np.array([int(n[header.index('Nodule')])])\n",
    "        \n",
    "        name = 'LNDb-{:04d}_finding{}_rad{}.jpg'.format(lnd,finding,rad)\n",
    "    if name in confirmed.index:\n",
    "\n",
    "        [scan,spacing,origin,transfmat] = readMhd('Train\\\\train\\\\LNDb-{:04}.mhd'.format(lnd))\n",
    "        transfmat_toimg,transfmat_toworld = getImgWorldTransfMats(spacing,transfmat)\n",
    "        ctr = convertToImgCoord(ctr,origin,transfmat_toimg)\n",
    "\n",
    "        # 2D slice with nodule center\n",
    "        scan_slice = scan[int(ctr[2])]\n",
    "        scan_slice_conv = gray_conv(scan_slice)\n",
    "        \n",
    "        # Previous slice\n",
    "        scan_slice_previous = scan[int(ctr[2])-1]\n",
    "        scan_slice_pre_conv = gray_conv(scan_slice_previous)\n",
    "        \n",
    "        # Next slice\n",
    "        scan_slice_next = scan[int(ctr[2])+1]\n",
    "        scan_slice_next_conv = gray_conv(scan_slice_next)\n",
    "                \n",
    "        R = np.stack((scan_slice_pre_conv, scan_slice_conv, scan_slice_next_conv), axis=2)\n",
    "        img = Image.fromarray(R)\n",
    "        img.save(\"yolov4/original/gray/LNDb-{:04d}_finding{}_rad{}.jpg\".format(lnd,finding,rad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions are used to find the new coordinates of the nodules after the (512,512) resize of the images\n",
    "def toPercentage(img_orig, x1, y1, x2, y2, xc, yc):\n",
    "    c, h, w = img_orig.shape\n",
    "    x1p = x1 / w\n",
    "    x2p = x2 / w\n",
    "    y1p = y1 / h\n",
    "    y2p = y2 / h\n",
    "    xcp = xc / w\n",
    "    ycp = yc / h\n",
    "    return x1p, y1p, x2p, y2p, xcp, ycp\n",
    "\n",
    "def toImCoord(x1p, y1p, x2p, y2p, xcp, ycp ):\n",
    "    h,w = (512, 512)\n",
    "    xmin = x1p * w\n",
    "    xmax = x2p * w\n",
    "    ymin = y1p * h\n",
    "    ymax = y2p * h \n",
    "    xcenter = xcp * w\n",
    "    ycenter = ycp * h\n",
    "    return xmin, xmax, ymin, ymax, xcenter, ycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolte coordinates of all nodules regardless their size\n",
    "def slice_info_wo(mask, slice_props, k):\n",
    "    y1, x1, y2, x2 = slice_props[k].bbox #x1 is xmin; x2 is xmax\n",
    "    yc_k, xc_k = slice_props[k].centroid\n",
    "\n",
    "    x1p, y1p, x2p, y2p, xcp, ycp = toPercentage(mask, x1, y1, x2, y2, xc_k, yc_k)\n",
    "    xmin, xmax, ymin, ymax, xcenter, ycenter = toImCoord(x1p, y1p, x2p, y2p, xcp, ycp)\n",
    "                \n",
    "    bh_k = ymax - ymin\n",
    "    bw_k = xmax - xmin\n",
    "\n",
    "    return xcenter, ycenter, bw_k, bh_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative to the image size coordinates - yolo compatible\n",
    "def slice_info_rel_wo(mask, slice_props, k):\n",
    "    y1, x1, y2, x2 = slice_props[k].bbox #x1 is xmin; x2 is xmax\n",
    "    yc_k, xc_k = slice_props[k].centroid\n",
    "\n",
    "    x1p, y1p, x2p, y2p, xcp, ycp = toPercentage(mask, x1, y1, x2, y2, xc_k, yc_k)\n",
    "    xmin, xmax, ymin, ymax, xcenter, ycenter = toImCoord(x1p, y1p, x2p, y2p, xcp, ycp)\n",
    "                \n",
    "    bh_k = ymax - ymin\n",
    "    bw_k = xmax - xmin\n",
    "        \n",
    "    return xcp, ycp, bw_k/512, bh_k/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all nodules' coordinates as values and the file name as a key\n",
    "dic_wo = defaultdict(dict)\n",
    "dic_rel_wo = defaultdict(dict)\n",
    "for mask_file in glob.glob('Train\\\\masks\\\\*.mhd'):\n",
    "    mask, spacing, origin, transfmat = readMhd(mask_file)\n",
    "    props = regionprops(mask, intensity_image=None, cache=False, coordinates=None)\n",
    "\n",
    "    for i in range(len(props)):\n",
    "        labels = props[i].label\n",
    "        zc, yc, xc = props[i].centroid\n",
    "\n",
    "        slice_props = regionprops(mask[int(zc)][:][:]) # check if there is another nodule on the first nodule's center slice\n",
    "                \n",
    "        file = mask_file.replace('mhd','jpg').split('_')\n",
    "                \n",
    "        dic_wo[str('{}_finding{}_{}'.format(file[0],labels,file[1]))]={k+1: slice_info_wo(mask, slice_props, k) for k in range (len(slice_props))}\n",
    "        dic_rel_wo[str('{}_finding{}_{}'.format(file[0],labels,file[1]))]={k+1: slice_info_rel_wo(mask, slice_props, k) for k in range (len(slice_props))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirmed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_positive = confirmed[confirmed.Nodule==1]\n",
    "confirmed_negative = confirmed[confirmed.Nodule==0]\n",
    "print(confirmed_positive.shape)\n",
    "print(confirmed_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_pos = confirmed_positive[\"LNDbID\"].values\n",
    "confirm_neg = confirmed_negative[\"LNDbID\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLO compatible .txt files with relative nodule coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k2, v2 in dic_rel_wo.items():\n",
    "    k2split = k2.rsplit(\".\")[0].rsplit('\\\\')[2]\n",
    "    if k2split in confirm_pos and v2[1] != None:\n",
    "        f= open(k2split  + \".txt\", \"w\")\n",
    "        for k, v in v2.items():\n",
    "            if k == 1 and v != None:\n",
    "                f.write(f\"{0} {v[0]} {v[1]} {v[2]} {v[3]}\")\n",
    "            if k > 1 and v != None:\n",
    "                f.write(f\"\\n{0} {v[0]} {v[1]} {v[2]} {v[3]}\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k2, v2 in dic_rel_wo.items():\n",
    "    k2split = k2.rsplit(\".\")[0].rsplit('\\\\')[2]\n",
    "    if k2split in confirm_neg:\n",
    "        f= open(k2split  + \".txt\", \"w\")\n",
    "        f.write(\"\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(confirmed_positive.diameter.values))\n",
    "print(np.min(confirmed_positive.diameter.values))\n",
    "print(np.max(confirmed_positive.diameter.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confirmed_positive.diameter.values, color='b', alpha=0.4)\n",
    "plt.title(\"Distribution of nodules' size\")\n",
    "plt.xlabel(\"Size in milimeters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomize patient wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_positive = confirmed_positive.reset_index()\n",
    "confirmed_negative = confirmed_negative.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_positive[\"path\"] = 'data/obj/' + confirmed_positive['LNDbID'] + \".jpg\"\n",
    "confirmed_negative[\"path\"] = 'data/obj/' + confirmed_negative['LNDbID'] + \".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group shuffle split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = np.array(confirmed_positive[\"path\"])\n",
    "y = np.array(confirmed_positive['Nodule'])\n",
    "groups = np.array(confirmed_positive[\"id\"])\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "gss.get_n_splits()\n",
    "\n",
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "       print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = confirmed_positive.iloc[train_idx]\n",
    "test = confirmed_positive.iloc[test_idx]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the train/test filenames in yolo compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = train['path'].values\n",
    "np.savetxt('train.txt', train_txt, fmt='%s', delimiter=',')\n",
    "test_txt = test['path'].values\n",
    "np.savetxt('test.txt', test_txt, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = readMhd('Train\\\\train\\\\LNDb-0024.mhd')[0]\n",
    "mask = readMhd('Train\\\\masks\\\\LNDb-0024_rad3.mhd')[0]\n",
    "props = regionprops(mask, intensity_image=None, cache=False, coordinates=None)\n",
    "\n",
    "for i in range(len(props)):\n",
    "    if i == 0:    \n",
    "        labels = props[i].label\n",
    "        print(\"Nodule: \", labels)\n",
    "        zc, yc, xc = props[i].centroid\n",
    "        zmin, y1, x1, zmax, y2, x2 = props[i].bbox #x1 is xmin; x2 is xmax\n",
    "        bw = x2 - x1\n",
    "        bh = y2 - y1\n",
    "        max_diff = max(zmax-zmin, y2-y1, x2-x1)\n",
    "        xy =(x1,y1)\n",
    "        print(\"Nodule coord: \", int(xc), int(yc), int(zc))\n",
    "        d = props[i].equivalent_diameter\n",
    "        print(\"Diameter: \", round(d))\n",
    "\n",
    "        fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "        ax[0].imshow(mask[int(zc)])\n",
    "        ax[0].add_patch(matplotlib.patches.Rectangle(xy, bw, bh, fill=False, ec = 'red'))\n",
    "        ax[1].imshow(scan[int(zc)]) #scan[int(zc)]\n",
    "        ax[1].add_patch(matplotlib.patches.Rectangle(xy, bw, bh, fill=False, ec = 'red'))\n",
    "\n",
    "        slice_props = regionprops(mask[int(zc)][:][:]) # check if there is another nodule on the first nodule's center slice\n",
    "        print(\"Number of subnodules on the same slice: \", len(slice_props) - 1)\n",
    "        print()         \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = readMhd('Train\\\\train\\\\LNDb-0011.mhd')[0]\n",
    "mask = readMhd('Train\\\\masks\\\\LNDb-0011_rad3.mhd')[0]\n",
    "props = regionprops(mask, intensity_image=None, cache=False, coordinates=None)\n",
    "print(\"Number of nodules: \", len(props))\n",
    "print()\n",
    "for i in range(len(props)):\n",
    "    if i == 0:    \n",
    "        labels = props[i].label\n",
    "        print(\"Nodule: \", labels)\n",
    "        zc, yc, xc = props[i].centroid\n",
    "        zmin, y1, x1, zmax, y2, x2 = props[i].bbox #x1 is xmin; x2 is xmax\n",
    "        bw = x2 - x1\n",
    "        bh = y2 - y1\n",
    "        max_diff = max(zmax-zmin, y2-y1, x2-x1)\n",
    "        xy =(x1,y1)\n",
    "        print(\"Nodule coord: \", int(xc), int(yc), int(zc))\n",
    "        d = props[i].equivalent_diameter\n",
    "        print(\"Diameter: \", round(d))\n",
    "\n",
    "        fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "        fig.tight_layout()\n",
    "        ax[0].imshow(mask[int(zc)])\n",
    "        ax[0].add_patch(matplotlib.patches.Rectangle(xy, bw, bh, fill=False, ec = 'red'))\n",
    "        ax[0].axis('off')\n",
    "        ax[0].set_title(\"Slice mask holding 1 nodule centroid & 2 more nodules\")\n",
    "        ax[1].imshow(scan[int(zc)]) #scan[int(zc)]\n",
    "        ax[1].add_patch(matplotlib.patches.Rectangle(xy, bw, bh, fill=False, ec = 'red'))\n",
    "        ax[1].axis('off')\n",
    "        ax[1].set_title(\"Raw slice with corresponding mask nodules\")\n",
    "\n",
    "        slice_props = regionprops(mask[int(zc)][:][:]) # check if there is another nodule on the first nodule's center slice\n",
    "        print(\"Number of subnodules on the same slice: \", len(slice_props) - 1)\n",
    "        print()\n",
    "\n",
    "        if len(slice_props) > 0:\n",
    "            for k in range(1,len(slice_props)):\n",
    "                d2 = slice_props[k].equivalent_diameter\n",
    "                print(\"Sub nodule diameter: \", round(d2))\n",
    "                if d2 > 0: #only greather than 3mm\n",
    "                    ymin, xmin, ymax, xmax = slice_props[k].bbox #x1 is xmin; x2 is xmax\n",
    "                    yc_k, xc_k = slice_props[k].centroid\n",
    "                    xy_k = (xmin, ymin)\n",
    "                    bh_k = ymax - ymin\n",
    "                    bw_k = xmax - xmin\n",
    "                    ax[0].add_patch(matplotlib.patches.Rectangle(xy_k, bw_k, bh_k, fill=False, ec = 'white'))\n",
    "                    ax[1].add_patch(matplotlib.patches.Rectangle(xy_k, bw_k, bh_k, fill=False, ec = 'white'))\n",
    "                    print(\"Properties x,y,w,h: \", int(xc_k), int(yc_k), int(bw_k), int(bh_k))\n",
    "                    print()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCube(scan,spacing,xyz,cube_size=80,cube_size_mm=51):\n",
    "    # Extract cube of cube_size^3 voxels and world dimensions of cube_size_mm^3 mm from scan at image coordinates xyz\n",
    "    xyz = np.array([xyz[i] for i in [2,1,0]],np.int)\n",
    "    spacing = np.array([spacing[i] for i in [2,1,0]])\n",
    "    scan_halfcube_size = np.array(cube_size_mm/spacing/2,np.int)\n",
    "    if np.any(xyz<scan_halfcube_size) or np.any(xyz+scan_halfcube_size>scan.shape): # check if padding is necessary\n",
    "        maxsize = max(scan_halfcube_size)\n",
    "        scan = np.pad(scan,((maxsize,maxsize,)),'constant',constant_values=0)\n",
    "        xyz = xyz+maxsize\n",
    "    \n",
    "    scancube = scan[xyz[0]-scan_halfcube_size[0]:xyz[0]+scan_halfcube_size[0], # extract cube from scan at xyz\n",
    "                    xyz[1]-scan_halfcube_size[1]:xyz[1]+scan_halfcube_size[1],\n",
    "                    xyz[2]-scan_halfcube_size[2]:xyz[2]+scan_halfcube_size[2]]\n",
    "\n",
    "    sh = scancube.shape\n",
    "    scancube = zoom(scancube,(cube_size/sh[0],cube_size/sh[1],cube_size/sh[2]),order=2) #resample for cube_size\n",
    "    \n",
    "    return scancube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvlines = readCsv('Train\\\\trainset_csv\\\\trainNodules.csv')\n",
    "header = csvlines[0]\n",
    "nodules = csvlines[1:2]\n",
    "\n",
    "for n in nodules:\n",
    "    lnd = int(n[header.index('LNDbID')])\n",
    "    rad = int(n[header.index('RadID')])\n",
    "    finding = int(n[header.index('FindingID')])\n",
    "\n",
    "    [scan,spacing,origin,transfmat] =  readMhd('Train\\\\train\\\\LNDb-{:04}.mhd'.format(lnd))\n",
    "    [mask,spacing,origin,transfmat] =  readMhd('Train\\\\masks\\\\LNDb-{:04}_rad{}.mhd'.format(lnd,rad))\n",
    "\n",
    "\n",
    "    if int(n[header.index('LNDbID')])==lnd and int(n[header.index('RadID')])==rad and int(n[header.index('FindingID')])==finding:\n",
    "        ctr = np.array([float(n[header.index('x')]), float(n[header.index('y')]), float(n[header.index('z')])])\n",
    "        \n",
    "    \n",
    "    # Convert coordinates to image\n",
    "        transfmat_toimg,transfmat_toworld = getImgWorldTransfMats(spacing,transfmat)\n",
    "        ctr = convertToImgCoord(ctr,origin,transfmat_toimg)\n",
    "        scan_cube = extractCube(scan,spacing,ctr)\n",
    "        scan_cube_central = normalize(scan_cube[int(scan_cube.shape[0]/2)])\n",
    "        mask[mask!=finding] = 0\n",
    "        mask[mask>0] = 1\n",
    "        mask_cube = extractCube(mask,spacing,ctr)\n",
    "        plt.figure(figsize = (8,3))\n",
    "        plt.imshow(scan_cube_central, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvlines = readCsv(\"yolov4\\\\data\\\\negative\\\\comp4_det_test_nodule_median.txt\")\n",
    "predic = csvlines[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_plot = []\n",
    "for line in predic:\n",
    "    line = line[0].split(\" \")\n",
    "    name = line[0] + \".jpg\"\n",
    "    if name not in img_to_plot: \n",
    "        img_to_plot.append(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in dic_wo.items():    \n",
    "    k = k.split(\"\\\\\")[2]\n",
    "    if k in img_to_plot:\n",
    "        print(k)\n",
    "        fig, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "        img = plt.imread(\"yolov4/median/negative/{}\".format(k))\n",
    "        ax.imshow(img)\n",
    "        for k2, v2 in v.items():\n",
    "            if v2 != None:\n",
    "                #print(v2)\n",
    "                xy = (v2[0]-(v2[2])/2, v2[1]-(v2[3]/2))\n",
    "                #print(xy)\n",
    "                bw = v2[2]\n",
    "                bh = v2[3]\n",
    "                ax.add_patch(matplotlib.patches.Rectangle(xy, bw, bh, fill=False, ec = 'yellow', lw = 1)) # ground truth\n",
    "        for line in predic:\n",
    "            line = line[0].split(\" \")\n",
    "            name = line[0] + \".jpg\"\n",
    "            if name == k and float(line[1]) >= 0.5:\n",
    "                w = (float(line[4]) - float(line[2]))\n",
    "                h = (float(line[5]) - float(line[3]))\n",
    "                xyp = (float(line[2]), float(line[3]))\n",
    "                ax.add_patch(matplotlib.patches.Rectangle(xyp, w, h, fill=False, ec = 'magenta', lw = 1))           \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
